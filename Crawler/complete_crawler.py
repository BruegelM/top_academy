#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤–µ–±-–∫—Ä–∞—É–ª–µ—Ä–∞
"""
import sqlite3
import json
import os
import signal
import psutil
from pathlib import Path
from datetime import datetime

def complete_crawler_execution():
    """–ó–∞–≤–µ—Ä—à–∞–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫—Ä–∞—É–ª–µ—Ä–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
    print("üîÑ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤–µ–±-–∫—Ä–∞—É–ª–µ—Ä–∞...")
    print("=" * 50)
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
    db_path = Path("crawler_data/crawler.db")
    if db_path.exists():
        try:
            with sqlite3.connect(db_path) as conn:
                cursor = conn.cursor()
                
                # –ü–æ–ª—É—á–∞–µ–º –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è travel.yandex.ru
                cursor.execute("""
                    SELECT id, domain FROM crawls 
                    WHERE status = 'in_progress' AND domain = 'travel.yandex.ru'
                """)
                active_crawls = cursor.fetchall()
                
                for crawl_id, domain in active_crawls:
                    print(f"üìä –ó–∞–≤–µ—Ä—à–∞–µ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ ID: {crawl_id}, –¥–æ–º–µ–Ω: {domain}")
                    
                    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    cursor.execute("SELECT COUNT(*) FROM pages WHERE crawl_id = ?", (crawl_id,))
                    total_pages = cursor.fetchone()[0]
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π
                    cursor.execute("""
                        UPDATE crawls 
                        SET end_time = ?, status = 'completed', total_pages = ?
                        WHERE id = ?
                    """, (datetime.now().isoformat(), total_pages, crawl_id))
                    
                    print(f"‚úÖ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ")
                    
                    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    export_results_from_db(cursor, crawl_id, domain)
                
                conn.commit()
                print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∞")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –ë–î: {e}")
    
    print("\nüéâ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫—Ä–∞—É–ª–µ—Ä–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print("üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ travel_yandex_results/")

def export_results_from_db(cursor, crawl_id, domain):
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –≤ JSON"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        cursor.execute("""
            SELECT url, depth, status_code, content_type, title, description,
                   is_external, links_count, images_count, parent_url
            FROM pages WHERE crawl_id = ?
            ORDER BY depth, url
        """, (crawl_id,))
        
        pages = cursor.fetchall()
        
        if pages:
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
            export_data = {
                'site_info': {
                    'domain': domain,
                    'total_pages': len(pages),
                    'crawl_id': crawl_id,
                    'completed_at': datetime.now().isoformat()
                },
                'pages': []
            }
            
            for page in pages:
                url, depth, status_code, content_type, title, description, is_external, links_count, images_count, parent_url = page
                export_data['pages'].append({
                    'url': url,
                    'depth': depth,
                    'status_code': status_code,
                    'content_type': content_type,
                    'title': title,
                    'description': description,
                    'is_external': bool(is_external),
                    'links_count': links_count,
                    'images_count': images_count,
                    'parent_url': parent_url
                })
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            results_dir = Path("travel_yandex_results")
            results_dir.mkdir(exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
            output_file = results_dir / "site_tree.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            
            print(f"üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {output_file}")
            print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(pages)}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            successful_pages = sum(1 for p in pages if p[2] == 200)  # status_code == 200
            error_pages = sum(1 for p in pages if p[2] and p[2] >= 400)
            max_depth = max(p[1] for p in pages) if pages else 0
            
            print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {successful_pages}")
            print(f"   ‚ùå –û—à–∏–±–∫–∏: {error_pages}")
            print(f"   üìè –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞: {max_depth}")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ: {e}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    try:
        complete_crawler_execution()
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  –û–ø–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()