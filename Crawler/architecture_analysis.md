# Архитектура веб-краулера для построения дерева сайта

## Анализ требований

### Основные функции:
1. **Сканирование сайта** - обход всех доступных URL на заданном домене
2. **Извлечение ссылок** - поиск и анализ всех ссылок на каждой странице
3. **Построение дерева** - создание иерархической структуры сайта
4. **Предотвращение циклов** - избежание повторного сканирования одних и тех же страниц
5. **Обработка ошибок** - корректная работа с недоступными страницами

### Технические требования:
- Поддержка HTTP/HTTPS протоколов
- Обработка различных типов контента (HTML, CSS, JS)
- Соблюдение robots.txt
- Ограничение скорости запросов (rate limiting)
- Многопоточность для ускорения процесса
- Возможность приостановки и возобновления сканирования

## Основные компоненты архитектуры

### 1. URL Manager (Менеджер URL)
**Назначение:** Управление очередью URL для сканирования
**Функции:**
- Добавление новых URL в очередь
- Получение следующего URL для обработки
- Отслеживание посещенных URL
- Фильтрация дублирующихся URL

### 2. Web Fetcher (Загрузчик веб-страниц)
**Назначение:** Загрузка содержимого веб-страниц
**Функции:**
- HTTP/HTTPS запросы
- Обработка заголовков ответа
- Управление таймаутами
- Обработка редиректов
- Соблюдение robots.txt

### 3. Content Parser (Парсер контента)
**Назначение:** Извлечение ссылок и метаданных из HTML
**Функции:**
- Парсинг HTML документов
- Извлечение всех типов ссылок (href, src, action)
- Нормализация URL
- Извлечение метаданных страницы (title, description)

### 4. Site Tree Builder (Построитель дерева сайта)
**Назначение:** Создание и поддержание структуры дерева сайта
**Функции:**
- Создание узлов дерева для каждой страницы
- Установление связей между страницами
- Определение иерархии страниц
- Вычисление глубины страниц

### 5. Crawler Controller (Контроллер краулера)
**Назначение:** Координация работы всех компонентов
**Функции:**
- Управление жизненным циклом сканирования
- Координация между компонентами
- Управление потоками выполнения
- Мониторинг прогресса

### 6. Data Storage (Хранилище данных)
**Назначение:** Сохранение результатов сканирования
**Функции:**
- Сохранение структуры дерева
- Кэширование результатов
- Экспорт в различные форматы
- Восстановление состояния

## Структура данных для дерева сайта

### Класс SiteNode (Узел сайта)
```python
class SiteNode:
    def __init__(self, url: str, parent: Optional['SiteNode'] = None):
        self.url: str = url
        self.parent: Optional['SiteNode'] = parent
        self.children: List['SiteNode'] = []
        self.metadata: Dict[str, Any] = {}
        self.status_code: Optional[int] = None
        self.content_type: Optional[str] = None
        self.last_crawled: Optional[datetime] = None
        self.depth: int = 0
        self.is_external: bool = False
```

### Класс SiteTree (Дерево сайта)
```python
class SiteTree:
    def __init__(self, root_url: str):
        self.root: SiteNode = SiteNode(root_url)
        self.nodes: Dict[str, SiteNode] = {root_url: self.root}
        self.domain: str = urlparse(root_url).netloc
```

## Алгоритм обхода URL

### Стратегии обхода:
1. **Breadth-First Search (BFS)** - обход в ширину
   - Преимущества: равномерное исследование всех уровней
   - Недостатки: большое потребление памяти

2. **Depth-First Search (DFS)** - обход в глубину
   - Преимущества: меньшее потребление памяти
   - Недостатки: может застрять на глубоких ветках

3. **Приоритетный обход**
   - Основан на важности страниц (PageRank, количество входящих ссылок)
   - Адаптивный подход

### Рекомендуемый алгоритм: Гибридный BFS
- Основа: обход в ширину
- Ограничение глубины для предотвращения бесконечного обхода
- Приоритизация важных страниц

## Методы извлечения ссылок

### Типы ссылок для извлечения:
1. **Навигационные ссылки** (`<a href="">`)
2. **Ресурсы** (`<img src="">`, `<link href="">`, `<script src="">`)
3. **Формы** (`<form action="">`)
4. **Фреймы** (`<iframe src="">`)
5. **CSS ссылки** (`@import`, `url()`)
6. **JavaScript ссылки** (динамически генерируемые)

### Нормализация URL:
- Преобразование относительных URL в абсолютные
- Удаление фрагментов (#)
- Нормализация параметров запроса
- Обработка кодировки URL

## Система управления очередью URL

### Компоненты очереди:
1. **Pending Queue** - URL ожидающие обработки
2. **Processing Set** - URL в процессе обработки
3. **Completed Set** - обработанные URL
4. **Failed Set** - URL с ошибками

### Приоритизация:
- Высокий приоритет: главная страница, sitemap.xml
- Средний приоритет: страницы верхнего уровня
- Низкий приоритет: глубоко вложенные страницы

## Механизм предотвращения циклов

### Методы обнаружения циклов:
1. **URL-based detection** - отслеживание посещенных URL
2. **Content-based detection** - сравнение хэшей контента
3. **Pattern-based detection** - обнаружение повторяющихся паттернов в URL

### Стратегии предотвращения:
- Максимальная глубина обхода
- Ограничение количества страниц с одного домена
- Blacklist для проблемных URL паттернов

## Обработка ошибок

### Типы ошибок:
1. **HTTP ошибки** (404, 500, 503)
2. **Сетевые ошибки** (timeout, connection refused)
3. **Парсинг ошибки** (невалидный HTML)
4. **Ограничения доступа** (robots.txt, 403)

### Стратегии обработки:
- Retry механизм с экспоненциальной задержкой
- Логирование всех ошибок
- Graceful degradation
- Уведомления о критических ошибках

## Система сохранения результатов

### Форматы экспорта:
1. **JSON** - для программного использования
2. **XML Sitemap** - для поисковых систем
3. **GraphML** - для визуализации графов
4. **CSV** - для анализа данных
5. **HTML Report** - для человеческого просмотра

### Структура JSON экспорта:
```json
{
  "site_info": {
    "domain": "example.com",
    "crawl_date": "2024-01-01T00:00:00Z",
    "total_pages": 150,
    "max_depth": 5
  },
  "tree": {
    "url": "https://example.com",
    "depth": 0,
    "children": [...]
  }
}